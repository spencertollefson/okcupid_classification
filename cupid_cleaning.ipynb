{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\",999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_values(df):\n",
    "    for i in df.columns:\n",
    "        print(\"\")\n",
    "        print(i,\":\", df[i].value_counts(),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas\n",
    "* Instead of horoscope sign, just do \"seriousness\" of horoscopes\n",
    "  * DONE\n",
    "* Religion column and \"faith intensity\" column? \"\" with horoscope\n",
    "  * DONE\n",
    "* Diet can be bifurcated ('stricts', 'mostlys', everything else)\n",
    "  * DONE\n",
    "* For SPEAKS - maybe English == English (fluent) Y/N, and then anything else fluent for a few languages?\n",
    "  * DONE\n",
    "* Word count, or verb count, for all essay columns\n",
    "* Should income be categorical or continuous?\n",
    "* Should I put NaNs in for all incomes of -1 (did not answer?). It's 48000+ observations...\n",
    "* Education: I think I want to group with highest level of achievement AND make groupings for current vs non-current student\n",
    "\n",
    "### Simple, basic transformations\n",
    "* **Drop** essays, last_online, drugs, drinks, diet, religion, speaks, location, ethnicity, offspring, sign\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_uniques(df):\n",
    "    for i in df.columns:\n",
    "        print(i,\":\", len(df[i].unique()))\n",
    "        \n",
    "def number_of_NaN(df):\n",
    "    for i in df.columns:\n",
    "        print(i,\":\", df[i].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Mandatory Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cupid = pd.read_csv('okcupid/profiles.csv')\n",
    "\n",
    "### Once ready for modeling, I should certainly drop:\n",
    "\n",
    "# cupid.drop(columns=['last_online','location'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spencer/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "/home/spencer/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    }
   ],
   "source": [
    "# Creating new columns\n",
    "cupid['diet_intensity'] = cupid.diet.str.extract('(\\w+ )')\n",
    "cupid.loc[ ( cupid.diet_intensity.isna() ) & ( cupid.diet.notna() ), 'diet_intensity'] = \"unanswered\"\n",
    "cupid['diet_choice'] = cupid.diet.str.split(\" \").str[-1]\n",
    "cupid['primary_ethnicity'] = cupid.ethnicity.str.replace(\", \",\",\").str.split(\",\",expand=True)[0]\n",
    "\n",
    "cupid['wants_kids'] = cupid.offspring.str.extract('(\\S*\\w*\\swant.*)', expand=True)[0]\n",
    "cupid['wants_kids'] = cupid.wants_kids.str.replace(\".*might.*\", \"maybe\",regex=True)\n",
    "cupid['wants_kids'] = cupid.wants_kids.str.replace(\".*does.*\", \"no\",regex=True)\n",
    "cupid['wants_kids'] = cupid.wants_kids.str.replace(\".*wants.*\", \"yes\",regex=True)\n",
    "cupid.loc[ ( cupid.wants_kids.isna() ) & ( cupid.offspring.notna() ), 'wants_kids'] = \"unanswered\"\n",
    "\n",
    "\n",
    "cupid['has_kids'] = cupid.offspring.str.extract('((\\S+\\s|^)(have|has)(\\s+\\S+|$))', expand=True)[0]\n",
    "cupid['has_kids'] = cupid.has_kids.str.replace(\".*does.*\", \"no\",regex=True)\n",
    "cupid['has_kids'] = cupid.has_kids.str.replace(\".*has a.*\", \"one\",regex=True)\n",
    "cupid['has_kids'] = cupid.has_kids.str.replace(\".*has kids.*\", \"multiple\",regex=True)\n",
    "cupid.loc[ ( cupid.has_kids.isna() ) & ( cupid.offspring.notna() ), 'has_kids'] = \"unanswered\"\n",
    "\n",
    "\n",
    "cupid['likes_cats'] = cupid.pets.str.contains(\"(^|\\s+)likes cats\", regex=True)\n",
    "cupid['likes_dogs'] = cupid.pets.str.contains(\"(^|\\s+)likes dogs\", regex=True)\n",
    "cupid['dislikes_cats'] = cupid.pets.str.contains(\"dislikes cats\")\n",
    "cupid['dislikes_dogs'] = cupid.pets.str.contains(\"dislikes dogs\")\n",
    "cupid['has_cats'] = cupid.pets.str.contains(\"has cats\")\n",
    "cupid['has_dogs'] = cupid.pets.str.contains(\"has dogs\")\n",
    "\n",
    "cupid['english_fluent'] = cupid.speaks.str.strip().str.extract(\"(english($|,|\\s\\(f))\")[0]\n",
    "cupid['english_fluent'] = cupid.english_fluent.str.replace('.*english.*', \"english\", regex=True)\n",
    "cupid.loc[ ( cupid.english_fluent.isna() ) & ( cupid.speaks.notna() ), 'english_fluent'] = \"placeholder\"\n",
    "\n",
    "cupid['english_poor'] = cupid.speaks.str.extract(\"(english \\([po])\")[0]\n",
    "cupid['english_poor'] = cupid.english_poor.str.replace('.*english.*', \"poor\", regex=True)\n",
    "cupid.loc[ ( cupid.english_poor.isna() ) & ( cupid.speaks.notna() ), 'english_poor'] = \"placeholder\"\n",
    "\n",
    "cupid['spanish_fluent'] =cupid.speaks.str.contains(\"spanish \\(f\", regex=True)\n",
    "cupid.loc[ ( cupid.spanish_fluent.isna() ) & ( cupid.speaks.notna() ), 'spanish_fluent'] = \"placeholder\"\n",
    "\n",
    "cupid['spanish_not_poorly'] = cupid.speaks.str.contains(\"spanish \\([po]\",regex=True)\n",
    "cupid.loc[ ( cupid.spanish_not_poorly.isna() ) & ( cupid.speaks.notna() ), 'spanish_not_poorly'] = \"placeholder\"\n",
    "\n",
    "cupid['signs_fun'] = cupid.sign.str.extract(\"(fun to think about)|(but it)|(matters a lot)\")[0]\n",
    "cupid.loc[ ( cupid.signs_fun.isna() ) & ( cupid.sign.notna() ), 'signs_fun'] = \"placeholder\"\n",
    "\n",
    "cupid['signs_unimportant'] = cupid.sign.str.extract(\"(fun to think about)|(but it)|(matters a lot)\")[1]\n",
    "cupid.loc[ ( cupid.signs_unimportant.isna() ) & ( cupid.sign.notna() ), 'signs_unimportant'] = \"placeholder\"\n",
    "\n",
    "cupid['signs_important'] = cupid.sign.str.extract(\"(fun to think about)|(but it)|(matters a lot)\")[2]\n",
    "cupid.loc[ ( cupid.signs_important.isna() ) & ( cupid.sign.notna() ), 'signs_important'] = \"placeholder\"\n",
    "\n",
    "cupid['religion_unserious'] = cupid.religion.str.extract(\"(not too serious)|(laughing)|(somewhat)|(very serious)\")[0]\n",
    "cupid.loc[ ( cupid.religion_unserious.isna() ) & ( cupid.religion.notna() ), 'religion_unserious'] = \"placeholder\"\n",
    "\n",
    "cupid['religion_laughing'] =cupid.religion.str.extract(\"(not too serious)|(laughing)|(somewhat)|(very serious)\")[1]\n",
    "cupid.loc[ ( cupid.religion_laughing.isna() ) & ( cupid.religion.notna() ), 'religion_laughing'] = \"placeholder\"\n",
    "\n",
    "cupid['religion_somewhat'] = cupid.religion.str.extract(\"(not too serious)|(laughing)|(somewhat)|(very serious)\")[2]\n",
    "cupid.loc[ ( cupid.religion_somewhat.isna() ) & ( cupid.religion.notna() ), 'religion_somewhat'] = \"placeholder\"\n",
    "\n",
    "cupid['religion_serious'] =cupid.religion.str.extract(\"(not too serious)|(laughing)|(somewhat)|(very serious)\")[3]\n",
    "cupid.loc[ ( cupid.religion_serious.isna() ) & ( cupid.religion.notna() ), 'religion_serious'] = \"placeholder\"\n",
    "\n",
    "cupid['religion_name'] = cupid.religion.str.extract(\"(\\w+)\")\n",
    "#### education changes\n",
    "cupid['new_education'] = cupid['education'].copy(deep=True)\n",
    "\n",
    "cupid.loc[cupid.new_education == \"high school\", 'new_education'] = \"graduated from high school\"\n",
    "cupid.loc[cupid.new_education == \"law school\", 'new_education'] = \"graduated from law school\"\n",
    "cupid.loc[cupid.new_education == \"masters program\", 'new_education'] = \"graduated from masters program\"\n",
    "cupid.loc[cupid.new_education == \"med school\", 'new_education'] = \"graduated from med school\"\n",
    "cupid.loc[cupid.new_education == \"ph.d program\", 'new_education'] = \"graduated from ph.d program\"\n",
    "cupid.loc[cupid.new_education == \"space camp\", 'new_education'] = \"graduated from space camp\"\n",
    "cupid.loc[cupid.new_education == \"two-year college\", 'new_education'] = \"graduated from two-year college\"\n",
    "cupid.loc[cupid.new_education == \"college/university\", 'new_education'] = \"graduated from college/university\"\n",
    "#### maybe I'll use these\n",
    "# cupid['education_droppedout'] = cupid.new_education.str.startswith(\"dropped out\")\n",
    "# cupid['education_graduated'] = cupid.new_education.str.startswith(\"graduated from\")\n",
    "# cupid['education_currentstudent'] = cupid.new_education.str.startswith(\"working on\")\n",
    "\n",
    "### create binary_smokes\n",
    "cupid['binary_smokes'] = cupid['smokes'].copy()\n",
    "cupid['binary_smokes'] = cupid.smokes.str.replace(\"(sometimes)|(when drinking)|(yes)|(trying to quit)\", \"yes\",regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=pd.io.pytables.PerformanceWarning)\n",
    "\n",
    "cupid.to_hdf('data/cupid_cleaned.hd5', key='df', mode='w')\n",
    "\n",
    "# import pickle\n",
    "# with open('data/cupid_cleaned.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(cupid, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_online: ['2012-06-28-20-30' '2012-06-29-21-41' '2012-06-27-09-10' ...\n",
      " '2012-06-02-08-16' '2012-02-17-20-44' '2012-06-14-16-51']\n"
     ]
    }
   ],
   "source": [
    "# print('smokes:',cupid.smokes.unique())\n",
    "# print('drinks:',cupid.drinks.unique())\n",
    "# print('drugs:',cupid.drugs.unique())\n",
    "print('last_online:',cupid.last_online.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cupid = cupid.drop(['essay0', 'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7', 'essay8', 'essay9',\n",
    "                    'last_online', 'drinks', 'drugs', 'diet', 'religion', 'speaks', 'location', 'ethnicity', 'offspring', 'sign'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
